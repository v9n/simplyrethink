-# Aggregation

Any database has to offer some kind of aggregation functions. RethinkDb is not
an exception here. 

The most popular function is `group`. Yeah, group is everywhere. Let's see how
it is handle in RethinkDB.

## group

  sequence.group(fieldOrFunction..., [{index: "indexName", multi: false}]) → grouped_stream

In a nut shell, taking a sequence, depend on the value of field or return value
of function, group document with same value into a group.

Looking at `flavors` table, let's group them by their `flavor_group` field:

    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
    #=>
    [{
        "group": "animal",
        "reduction": [
            {
                "category": "odor",
                "created_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561018,
                    "timezone": "-07:00"
                },
                "creator_id": null,
                "flavor_group": "animal",
                "id": 112,
                "name": "animal",
                "updated_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561018,
                    "timezone": "-07:00"
                },
                "updater_id": null
            }
        ]
    },
    {
        "group": "balsamic",
        "reduction": [
            {
                "category": "odor",
                "created_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561011,
                    "timezone": "-07:00"
                },
                "creator_id": null,
                "flavor_group": "balsamic",
                "id": 43,
                "name": "others",
                "updated_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561011,
                    "timezone": "-07:00"
                },
                "updater_id": null
            },
            {
                "category": "odor",
                "created_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561010,
                    "timezone": "-07:00"
                },
                "creator_id": null,
                "flavor_group": "balsamic",
                "id": 40,
                "name": "chocolate",
                "updated_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561010,
                    "timezone": "-07:00"
                },
                "updater_id": null
            }
        ]
    },
    {
        "group": "camphoraceous",
        "reduction": [
            {
                "category": "odor",
                "created_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561017,
                    "timezone": "-07:00"
                },
                "creator_id": null,
                "flavor_group": "camphoraceous",
                "id": 101,
                "name": "camphoraceous",
                "updated_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561017,
                    "timezone": "-07:00"
                },
                "updater_id": null
            }
        ]
    },
    ...]

The returned array includes two field:

  * group: value of group field, in our case, value of `flavor_group` field
  * reduction: an array contains all of document has same value for `flavor_group` field

When we continue to chain function after `group`, the function will operate on `reduction` array.
Such as we can count how many element of **reduction**:

    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
      .count()
    //=>
    [
    {
    "group": null ,
    "reduction": 743
    } ,
    {
    "group":  "animal" ,
    "reduction": 1
    } ,
    {
    "group":  "balsamic" ,
    "reduction": 10
    } ,
    {
    "group":  "camphoraceous" ,
    "reduction": 1
    } ,
    ...
    ]

T> ## Command chain after group runs on grouped array
T> 
T> It's important to understand `group` make next function call operator
T> on its `reduction` field.

How about group them, and get the first document of grouped stream.

    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
      .nth(0)
    //=>
    [
    {
    "group": null ,
    "reduction": {
    "category":  "odor" ,
    "created_at": Sun Oct 02 2011 06:12:18 GMT-07:00 ,
    "creator_id": null ,
    "flavor_group": null ,
    "id": 148 ,
    "name":  "cotton candy" ,
    "updated_at": Sun Oct 02 2011 06:12:18 GMT-07:00 ,
    "updater_id": null
    }
    } ,
    {
    "group":  "animal" ,
    "reduction": {
    "category":  "odor" ,
    "created_at": Sun Oct 02 2011 06:10:18 GMT-07:00 ,
    "creator_id": null ,
    "flavor_group":  "animal" ,
    "id": 112 ,
    "name":  "animal" ,
    "updated_at": Sun Oct 02 2011 06:10:18 GMT-07:00 ,
    "updater_id": null
    }
    } ,
    {
    "group":  "balsamic" ,
    "reduction": {
    "category":  "odor" ,
    "created_at": Sun Oct 02 2011 06:10:10 GMT-07:00 ,
    "creator_id": null ,
    "flavor_group":  "balsamic" ,
    "id": 40 ,
    "name":  "chocolate" ,
    "updated_at": Sun Oct 02 2011 06:10:10 GMT-07:00 ,
    "updater_id": null
    }
    } ,
    ...
    ]

W> Why null group?
W>
W> Why do we have a value ***null*** here. It's because 
W> some document doesn't have any value for `flavor_group`,
W> or in other word, NULL value. They are put into same group
W> of NULL

Note that we have some limit with `group` where the `group` size is 
over 100000 element. Example, let's group `compounds_foods` by their
`orig_food_common_name`

    r.db("foodb")
      .table("compounds_foods")
      .group('orig_food_common_name')

And we got this:

    RqlRuntimeError: Grouped data over size limit `100000`.  Try putting a reduction (like `.reduce` or `.count`) on the end in:
      r.db("foodb").table("compounds_foods").group("orig_food_common_name")

Why so? because when we end the chain with `group`, the whole array is loaded
into memory, and our sequence are greater than 100000 element. We have around
~668K document. However, when we call `reduce` or `count` on it, RethinkDb won't
load them all into memory and make it works. I don't know why, honestly, I read
all of this from this issue [^issue2596]

[^issue2596]: https://github.com/rethinkdb/rethinkdb/issues/2596

Let's try what it suggest:

    r.db("foodb")
      .table("compounds_foods")
      .group('orig_food_common_name')
      .count()
    #=>
    //Executed in 45.03s. 9492 rows returned
    [
    {
        "group": null,
        "reduction": 4313
    },
    {
        "group": "AMARANTH FLAKES",
        "reduction": 68
    },
    ...
    ]

Now, this result is of course not what we expect. But why it runs? It is because when we call `count`, the `reduction` field become a single
value instead of an array of grouped data, that makes the size of final group data smaller. As you can see, ***9492*** is returned, and
that's all loaded into memory.

So keep in mind that we have some limitation with `group`, at least at this moment.


## ungroup

As you see, anything follow `group` operates on sub stream, or `reduction` array.
Can we make the follow function run on returned sequence of `group` itself? Such as,
we want to sort by the value of `reduction` field. Let's try to sort `flavors` by
how many value of `flavor_group`

    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
      .count()
      .ungroup()
      .orderBy(r.desc('reduction'))
    //=>
    [
    {
    "group": null ,
    "reduction": 743
    } ,
    {
    "group":  "fruity" ,
    "reduction": 24
    } ,
    {
    "group":  "floral" ,
    "reduction": 14
    } ,
    {
    "group":  "balsamic" ,
    "reduction": 10
    },...
    ]

So `ungroup` turns the return array from `group` into a sequence of object, with each object
includes 2 fields:

    * group
    * reduciton

and let any chain command follow it operator on this sequence.

## reduce

I think at this point, you already know what `count()` does. From a sequence, or an array,
it returns a single number of how many items the sequence holds. So it transform a whole
array into a single value. Not similar to map, `map` turn each of element of sequence into
other value, and return a new sequence with all return value from `map` function. 

`count` is an example of `reduce`. `reduce` accepts a function, let's call it `reduce_function`,
and produces a single value, by repeating a function with input is the previous output of 
`reduce_function`. The `reduce_function` can be called with those parameters:

  * two element of sequence
  * one element of sequence and one result of previous reduce_function execution
  * two results of previous reductions.

We can guess, and somehow say that, on the first execution, two first elements of 
sequence are passed into reduce function, on the second execution, one parameter is
third element of sequence, other parameter is the result of reduce function call on
first and second element. And so on for 4th, 5h execution...

But why do we have two results of previous reductions? It's because reduce function 
can run parallel across shards and CPU core, or even across computers in a cluster.
The final result of each reduce functions on each shards, or each computer, finally
are passed to reduce function again, to create final result.

But what happens if the sequence has a single element? We don't have enough input
for reduce function. That is a special case, and RethinkDB will simply return 
value of element as result of reduce function.

Usually, `reduce` will be used with `map` to transform document into a value that can 
be aggreation. As you saw, the reduce function paramters can be element of sequence, 
or the result of previous reduce function. Therefore, we will usually needs `map` to
do some transformation so that the type of parameters and result of reduce function are
same.

T> Tail call optimization
T>
T> You may already sense that `reduce` is similar to recursion. But recursion is bad, they
T> are quicjky fill up the heap. However, you can notice that we are also passing return
T> value to next call of `reduce` function. Therefore the reduce function doesn't need to
T> know about state of previous call. Or, in other words, we don't have to store a stack
T> for previous function call. That's tail call optimization where we can reuse
T> current stack frame for function call instead of creating new one.
T> I think that under the hood, reduce function is implement in that sense.

Let's rewrite `count` in `reduce` style. Giving it a bit thinking, the way we count thing
is that we have an variable, each time we get a new input, we increment that variable 1,
until all of input are process. The final value is how many element we have. Such as,
counting in JavaScript:

    function count(sequence) {
      sum = 0
      for (i=0; i<=sequence.length; i++) {
        sum = sum + 1
      }
      return sum
    }
    count(['a', 'b', 'a1', 12, 'a9', 13])

In above ***count*** function of JavaScript, we have a step where we initialize the
value of `sum` to 0, and a step where we increment `sum` to 1. 

Let's re-write it in recursion way so we can simulate `reduce` style:

    function count(element, sum) {
      if (typeof element == 'undefined') {
        return 
      }

      if (typeof element == 'undefined') {
        return 0
      }
      if (typeof sum == 'undefined') {
        sum = 0
      }
      count(
    }

    for (i=0; i<=sequence.length; i++) {
      
    }
    
    reduce_function = function(left, right) {
      return left+right
    }
    count_with_reduce(sequence, reduce_function(left, right) {
    })

    if (typeof sequence[0] == 'undefined') {
      return 0
    }

    if (typeof sequence[1] == 'undefined') {
      return 1
    }

    

## distinct

Given an sequence, `distinct` remove duplication from it. When giving an index,
the duplication is detected by value of index. Its syntax is:

    sequence.distinct() → array
    table.distinct([:index => <indexname>]) → stream

As you can tell, whenever we return an array, we will run into 100,000 element
isues if the return array has more than 100,000 element. So keep that in mind,
and try to call distinct with a proper index.

Let's count how many distinct `user` we have:

    r.db("foodb")
      .table("users")
      .withFields("name")
      .distinct()
    //=>Executed in 30ms. 152 rows returned
    [
    {
    "name":  "Abe Willms"
    } ,
    {
    "name":  "Adela Klein V"
    } ,...]


Let's see what kind of `age` our users are:

    r.db("foodb")
      .table("users")
      .withFields("age")
      .distinct()
   //=> 71 rows are returned
    [
    {
    "age": 9
    } ,
    {
    "age": 10
    } ,
    {
    "age": 13
    } ,
    {
    "age": 14
    } ,]

So we have 152 unique user's name, and 71 unique `age`. 


What happens when using an index?  Let's try to create an index for user age, we 
divide them into 3 groups:

    r.db("foodb")
      .table("users")
      .indexCreate("age-group", function (user) {
        return
          r.branch(
            user("age").lt(18),
            "teenager",
            r.branch(user("age").gt(50),
              "older",
              "adult"
            )
          )   
      })

Calling `distinct` on table, passing that index:

    r.db("foodb")
      .table("users")
      .distinct({index: "age-group"})
    //=> 3 rows return
    "adult"
    "older"
    "teenager"

So when passing index, the value of index is returned and therefore that value is
used to detect duplication.

## count

You already knew `count`. It also has an extra useful way to count is passing a
value or a function. When passing a value, it counts the document whose value matches
the value. Such as counting users who are 18 year olds:

    r.db("foodb")
      .table("users")("age")
      .count(18)

When passing a function, it counts the document whose function evaulation returns true
on document, very similar to filter function. But filter returns document, and `count`
counts them. Such as counting user who are 19 years old and name stars with K:

    r.db("foodb")
      .table("users")
      .count(function(user) {
        return user("age").eq(23).and(user("name").match("^L"))
      })
    //=>
    1

We can use `row` function too:

    r.db("foodb")
      .table("users")
      .count(r.row("rowage").eq(23).and(r.row("name").match("^L")))
    //=>
    1

## Map Reduce

Since RethinkDB call itself `The open-source database for the realtime web`, it
has support for map-reduce. Map reduce is a way to aggregation a large data set,
run parallel on many servers, since RethinkDB is distributed.`, it has support
for map-reduce. Map reduce is a way to aggregation a large data set, run
parallel on many servers, since RethinkDB is distributed.

Sometimes 

# Wrap up

You know how to group data, set a default value
