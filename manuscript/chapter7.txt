-# Aggregation

Any database has to offer some kind of aggregation functions, and RethinkDb is not
an exception here.

## sum, average, and count

A very normal task of a database is to get some kind of calculate from a given
sequence of data. You already know `count`, by calling it on a stream, you get
how many document it has. You can also passing a value or a function to `count`
so RethinkDB only count document match the value or when predicate function
return true.

For example, let count how many food is nut we have

    r.db('foodb').table('foods')('food_type').count('Type 1')
    //=>
    627

Here we are using bracket to fetch *food_type* field and count how many value
that match *Tyepe 1*. We can also pass a ReQL expression or a function. Let's
find all food whose name starts with *L*

    r.db('foodb').table('foods').count(r.row('name').match('^L'))
    //=>
    31

Using function:

    r.db('foodb').table('foods')
      .count(function(food){
        return food('name').match('^L')
        })
    //=>
    31

With some smart combination with `map`, we can `count` in different way:

    r.db('foodb').table('foods').map(r.row('name').split('').nth(0)).count('L')

Basically we get food name, split charater one by one using `split` command,
then call `nth(0)` to fet first document. Using `map` we transform food name
table into a stream of first charater of food name, then we count this stream
for how many element equal *L*.

`sum`, and `averag` is similar to `count` on how you use them. Just different
on what they give you. `sum` give you sum of sequence, and `average` does 
*average*. Let's find out how many bytes of storage need to store food image.
Each of document in *foods* table has a *picture_file_size* field store in
byte. 

    r.db('foodb').table('foods').sum('picture_file_size')
    //=>
    123463051

We can also sum direcly on the value of stream

    r.db('foodb').table('foods')('picture_file_size').sum()

The key thing is that you understand how those function operate. By default,
they operator on the whole document. That's why we have to use `sum('pictire_file_size')`
when we call `sum` direcly on table. However, when we already use bracket to
get the field, we can simply call `sum()` without any parameters.

    r.db('foodb').table('foods')('picture_file_size').sum()

So you can guess what is the average file size, let's find out it:

    r.db('foodb').table('foods')('picture_file_size').avg()
    //=>
    147155.0071513707

We can also passing a function to `sum` or `avg`. In that case, RethinkDb calls
the function on every document, then get the result and use them for sum
purpose.

Let's say we only interested in filesize which is bigger than 4MB

    r.db('foodb').table('foods').sum(function(food) {
      return r.branch(
       food('picture_file_size').gt(1024*1024*4),
       food('picture_file_size'),
        0)
    })
    //=>
    9666379


The most popular function is `group`. Yeah, group is everywhere. Let's see how
it is handle in RethinkDB.

## group

    sequence.group(fieldOrFunction..., [{index: "indexName", multi: false}]) → grouped_stream

In a nut shell, taking a sequence, depend on the value of field or return value
of function, we can group document with same value into a group.

Looking at `flavors` table, let's group them by their `flavor_group` field:

    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
    #=>
    [{
        "group": "animal",
        "reduction": [
            {
                "category": "odor",
                "created_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561018,
                    "timezone": "-07:00"
                },
                "creator_id": null,
                "flavor_group": "animal",
                "id": 112,
                "name": "animal",
                "updated_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561018,
                    "timezone": "-07:00"
                },
                "updater_id": null
            }
        ]
    },
    {
        "group": "balsamic",
        "reduction": [
            {
                "category": "odor",
                "created_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561011,
                    "timezone": "-07:00"
                },
                "creator_id": null,
                "flavor_group": "balsamic",
                "id": 43,
                "name": "others",
                "updated_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561011,
                    "timezone": "-07:00"
                },
                "updater_id": null
            },
            {
                "category": "odor",
                "created_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561010,
                    "timezone": "-07:00"
                },
                "creator_id": null,
                "flavor_group": "balsamic",
                "id": 40,
                "name": "chocolate",
                "updated_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561010,
                    "timezone": "-07:00"
                },
                "updater_id": null
            }
        ]
    },
    {
        "group": "camphoraceous",
        "reduction": [
            {
                "category": "odor",
                "created_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561017,
                    "timezone": "-07:00"
                },
                "creator_id": null,
                "flavor_group": "camphoraceous",
                "id": 101,
                "name": "camphoraceous",
                "updated_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561017,
                    "timezone": "-07:00"
                },
                "updater_id": null
            }
        ]
    },
    ...]

The returned array includes two fields:

  * group: value of group field, in our case is value of `flavor_group` field
  * reduction: an array contains all of document has same value for `flavor_group` field

When we continue to chain function after `group`, the function will operate on `reduction` array.
For example, we can count how many element of **reduction**:

    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
      .count()
    //=>
    [
    {
    "group": null ,
    "reduction": 743
    } ,
    {
    "group":  "animal" ,
    "reduction": 1
    } ,
    {
    "group":  "balsamic" ,
    "reduction": 10
    } ,
    {
    "group":  "camphoraceous" ,
    "reduction": 1
    } ,
    ...
    ]

T> ## Command chain after group runs on grouped array
T> 
T> It's important to understand `group` make next function call operator
T> on its `reduction` field.

How about group them,and get the first document of grouped stream?

    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
      .nth(0)
    //=>
    [
    {
    "group": null ,
    "reduction": {
    "category":  "odor" ,
    "created_at": Sun Oct 02 2011 06:12:18 GMT-07:00 ,
    "creator_id": null ,
    "flavor_group": null ,
    "id": 148 ,
    "name":  "cotton candy" ,
    "updated_at": Sun Oct 02 2011 06:12:18 GMT-07:00 ,
    "updater_id": null
    }
    } ,
    {
    "group":  "animal" ,
    "reduction": {
    "category":  "odor" ,
    "created_at": Sun Oct 02 2011 06:10:18 GMT-07:00 ,
    "creator_id": null ,
    "flavor_group":  "animal" ,
    "id": 112 ,
    "name":  "animal" ,
    "updated_at": Sun Oct 02 2011 06:10:18 GMT-07:00 ,
    "updater_id": null
    }
    } ,
    {
    "group":  "balsamic" ,
    "reduction": {
    "category":  "odor" ,
    "created_at": Sun Oct 02 2011 06:10:10 GMT-07:00 ,
    "creator_id": null ,
    "flavor_group":  "balsamic" ,
    "id": 40 ,
    "name":  "chocolate" ,
    "updated_at": Sun Oct 02 2011 06:10:10 GMT-07:00 ,
    "updater_id": null
    }
    } ,
    ...
    ]

W> Why null group?
W>
W> Why do we have a value ***null*** here. It's because 
W> some documents don't have any value for `flavor_group`,
W> or in other words, a NULL value. They are put into the same group
W> of NULL

Note that we have some limitations with `group` where the `group` size is 
over 100000 elements. For example, let's group `compounds_foods` by their
`orig_food_common_name`

    r.db("foodb")
      .table("compounds_foods")
      .group('orig_food_common_name')

And we got this:

    RqlRuntimeError: Grouped data over size limit `100000`.  Try putting a reduction (like `.reduce` or `.count`) on the end in:
      r.db("foodb").table("compounds_foods").group("orig_food_common_name")

Why so? Because when we end the chain with `group`, the whole array is loaded
into memory, and our sequence are greater than 100000 elements. We have around
~668K documents. However, when we call `reduce` or `count` on it, RethinkDb won't
load them all into memory and makes it works. Honestly, I don't know why, I've only read about 
all of this from this issue [^issue2596]

[^issue2596]: https://github.com/rethinkdb/rethinkdb/issues/2596

Let's try what it suggests:

    r.db("foodb")
      .table("compounds_foods")
      .group('orig_food_common_name')
      .count()
    #=>
    //Executed in 45.03s. 9492 rows returned
    [
    {
        "group": null,
        "reduction": 4313
    },
    {
        "group": "AMARANTH FLAKES",
        "reduction": 68
    },
    ...
    ]

Now, this result is of course not what we expected. But why it runs? It is because when we call `count`, the `reduction` field becomes a single
value instead of an array of grouped data, that makes the size of final group data smaller. As you can see, ***9492*** is returned, and
that's all loaded into memory.

So keep in mind that we have some limitation with `group`, at least at this moment.


## ungroup

As you can see, anything follow `group` operates on sub stream, or `reduction` array.
Can we make the follow function run on returned sequence of `group` itself? Such as,
we want to sort by the value of `reduction` field. Let's try to sort `flavors` by
how many value of `flavor_group`

    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
      .count()
      .ungroup()
      .orderBy(r.desc('reduction'))
    //=>
    [
    {
    "group": null ,
    "reduction": 743
    } ,
    {
    "group":  "fruity" ,
    "reduction": 24
    } ,
    {
    "group":  "floral" ,
    "reduction": 14
    } ,
    {
    "group":  "balsamic" ,
    "reduction": 10
    },...
    ]

So `ungroup` turns the return array from `group` into a sequence of object, with each object
includes 2 fields:

    * group
    * reduciton

and let any chain command follows it operate on this sequence.

Let's say we want to find top 10 fruits which has most flavors. Because we want
to eat fruits that is tasty, right?

We have a table *compounds_foods* with *food_id* and *compound_id*. Using
*food_id* we can know the food. Using *compound_id* we can know how many
flavors it has by querying on *compounds_flavors* table. Here we only care
abotut how many flavors, so we only needs to count, no need to get flavor
detail.



## reduce

I think at this point, you already know what `count()` does. From a sequence, or an array,
it returns a single number of how many items the sequence holds. So it transforms a whole
array into a single value. Unlike `map` which turns each element of sequence into
other value, and returns a new sequence with all return value from `map` function. 

`count` is an example of `reduce`. `reduce` accepts a function, let's call it `reduce_function`,
and produces a single value, by repeating a function with input is the previous output of 
`reduce_function`. The `reduce_function` can be called with those parameters:

  * two elements of sequence
  * one element of sequence and one result of previous reduce_function execution
  * two results of previous reductions.

We can say that, on the first execution, two first elements of 
sequence are passed into reduce function; on the second execution, one parameter is
third element of sequence, other parameter is the result of reduce function call on
first and second element. And so on for 4th, 5h execution...

But why do we have two results of previous reductions? It's because reduced function 
can run parallel across shards and CPU core, or even across computers in a cluster.
The final result of each reduce functions on each shards or each computer,
are then passed to reduce function again, to create final result.

But what will happen if the sequence has a single element? We don't have enough input
for reduce function. That is a special case, and RethinkDB will simply return 
value of element as result of reduce function.

Usually, `reduce` will be used with `map` to transform document into a value that can 
be aggregated. As you have seen, the reduce function paramters can be element of sequence, 
or the result of previous reduce function. Therefore, we will need `map` to
do some transformation so that the type of parameters and result of reduce function are
the same.

T> Tail call optimization
T>
T> You may already sense that `reduce` is similar to recursion. But recursion is bad, they
T> are quicjky fill up the heap. However, you know that we are also passing return
T> value to next call of `reduce` function. Therefore, the reduce function doesn't need to
T> know about state of a previous call. In other words, we don't have to store a stack
T> for previous function call. That's tail called optimization where we can reuse
T> current stack frame for function call instead of creating new one.
T> I think that under the hood, reduce function is implemented in that sense.

Let's rewrite `count` in `reduce` style. Giving it some thoughts, you know that the way we count thing
is by using variable; each time we get a new input, we increment that variable by 1,
until all of input are processed. The final value is how many element we have such as
counting in JavaScript:

    function count(sequence) {
      sum = 0
      for (i=0; i<=sequence.length; i++) {
        sum = sum + 1
      }
      return sum
    }
    count(['a', 'b', 'a1', 12, 'a9', 13])

In the above ***count*** function of JavaScript, we have a step where we initialize the
value of `sum` to 0, and a step where we increment `sum` to 1. 

Let's re-write it in a recursion way, so we can simulate `reduce` style:

    function count(element, sum) {
      if (typeof element == 'undefined') {
        return 
      }

      if (typeof element == 'undefined') {
        return 0
      }
      if (typeof sum == 'undefined') {
        sum = 0
      }
      count(
    }

    for (i=0; i<=sequence.length; i++) {
      
    }
    
    reduce_function = function(left, right) {
      return left+right
    }
    count_with_reduce(sequence, reduce_function(left, right) {
    })

    if (typeof sequence[0] == 'undefined') {
      return 0
    }

    if (typeof sequence[1] == 'undefined') {
      return 1
    }

    

## distinct

Given a sequence, `distinct` remove duplication from it. When giving an index,
the duplication is detected by value of index. Its syntax is:

    sequence.distinct() → array
    table.distinct([:index => <indexname>]) → stream

As you can tell, whenever we return an array, we will run into 100,000 element
isues if the return array has more than 100,000 elements. So keep that in mind
and try to call distinct with a proper index.

Let's count how many distinct `user` we have:

    r.db("foodb")
      .table("users")
      .withFields("name")
      .distinct()
    //=>Executed in 30ms. 152 rows returned
    [
    {
    "name":  "Abe Willms"
    } ,
    {
    "name":  "Adela Klein V"
    } ,...]


Let's see what kind of `age` our users are:

    r.db("foodb")
      .table("users")
      .withFields("age")
      .distinct()
   //=> 71 rows are returned
    [
    {
    "age": 9
    } ,
    {
    "age": 10
    } ,
    {
    "age": 13
    } ,
    {
    "age": 14
    } ,]

So we have 152 unique user's name and 71 unique `age`. 


What will happen when using an index?  Let's try to create an index for user age, we 
divide them into 3 groups:

    r.db("foodb")
      .table("users")
      .indexCreate("age-group", function (user) {
        return
          r.branch(
            user("age").lt(18),
            "teenager",
            r.branch(user("age").gt(50),
              "older",
              "adult"
            )
          )   
      })

Calling `distinct` on table, passing that index:

    r.db("foodb")
      .table("users")
      .distinct({index: "age-group"})
    //=> 3 rows return
    "adult"
    "older"
    "teenager"

So when passing index, the value of index is returned; and therefore that value is
used to detect duplication.

## count

You already knew `count`. It also has an extra useful way to count is by passing a
value or a function. When passing a value, it counts the document whose value matches
the value such as counting users who are 18 year olds:

    r.db("foodb")
      .table("users")("age")
      .count(18)

When passing a function, it counts the document whose function evaulation returns true
on document, very similar to filter function. However, filter returns document while `count`
counts them such as counting users who are 19 years old and name starts with a K:

    r.db("foodb")
      .table("users")
      .count(function(user) {
        return user("age").eq(23).and(user("name").match("^L"))
      })
    //=>
    1

We can use `row` function too:

    r.db("foodb")
      .table("users")
      .count(r.row("rowage").eq(23).and(r.row("name").match("^L")))
    //=>
    1

We also have some other aggregation functions such as `sum`, `min`, `max` but
they are very simple, and you can read from API page in aggregation section[^Aggregation].

[^Aggregation]: http://www.rethinkdb.com/api/javascript/max/#

# Wrap up

When finishing this chapter, you should know how to aggregation, how to group data, counting,
and call function on grouped data.
