-# Aggregation

A very normal task of a database is to get some kind of calculate from a given
sequence of data. We will learn those kind of function on this chapter.

## sum, average, and count

You already knew `count`. It also has an extra useful way to count is by passing a
value or a function. When passing a value, it counts the document whose value matches
the value such as counting users who are 18 year olds:

For example, let count how many food is *Type 1* we have

    r.db('foodb').table('foods')('food_type').count('Type 1')
    //=>
    627

Here we are using nested field syntax to fetch *food_type* field and count how many value that match *Type 1*.

Or count how many user who are *18* years old.

    r.db("foodb")
      .table("users")("age")
      .count(18)

We can also pass a ReQL expression or a function. Let's
find all food whose name starts with *L*

    r.db('foodb').table('foods').count(r.row('name').match('^L'))
    //=>
    31

We can also passing a value or a function to `count`
so RethinkDB only count documents match the value or when predicate function
returns true.

    r.db('foodb').table('foods')
      .count(function(food){
        return food('name').match('^L')
        })
    //=>
    31

In RethinkDB, it's very flexible on how we do thing. Such as, counting, in
fundamental is just count the element from an array. With some smart
combination we can `count` same thing in different way:

    r.db('foodb').table('foods').map(r.row('name').split('').nth(0)).count('L')

Basically we get food name, split charater one by one using `split` command,
then call `nth(0)` to fet first document. Using `map` we transform food name
table into a stream of first charater of food name, then we count this stream
for how many element equal *L*.

In a sense, passing a function is like a shortcut of filter with that function,
and count the return sequence

Below example, counting users who are 19 years old and name starts with a K:

    r.db("foodb")
      .table("users")
      .count(function(user) {
        return user("age").eq(23).and(user("name").match("^L"))
      })
    //=>
    1

If we run `filter`, before `count`:

    r.db("foodb")
      .table("users")
      .filter(function(user) {
        return user("age").eq(23).and(user("name").match("^L"))
      })
      .count()
    //=>
    1

We got same result but it feel very redundant.

We can also pass a ReQL expression which is evaluate to true or false:

    r.db("foodb")
      .table("users")
      .count(r.row("rowage").eq(23).and(r.row("name").match("^L")))
    //=>
    1

`sum`, and `averag` is similar to `count` on how you use them. Just different
on what they give you. `sum` give you sum of sequence, and `average` does 
*average*. Let's find out how many bytes of storage need to store food image.
Each of document in *foods* table has a *picture_file_size* field store in
byte.

    r.db('foodb').table('foods').sum('picture_file_size')
    //=>
    123463051

We can also sum direcly on the value of stream

    r.db('foodb').table('foods')('picture_file_size').sum()

The key thing is that you understand how those function operate. By default,
they operator on the whole document. That's why we have to use `sum('pictire_file_size')`
when we call `sum` direcly on table. However, when we already use bracket to
get the field, we can simply call `sum()` without any parameters.

    r.db('foodb').table('foods')('picture_file_size').sum()

So you can guess what is the average file size, let's find out it:

    r.db('foodb').table('foods')('picture_file_size').avg()
    //=>
    147155.0071513707

We can also passing a function to `sum` or `avg`. In that case, RethinkDb calls
the function on every document, then get the result and use them for sum
purpose.

Let's say we only interested in filesize which is bigger than 4MB

    r.db('foodb').table('foods').sum(function(food) {
      return r.branch(
       food('picture_file_size').gt(1024*1024*4),
       food('picture_file_size'),
        0)
    })
    //=>
    9666379

Here, we are using `branch` as a normal `if else` block:

    if food('picture_file_size') > 1024 * 1024 * 4
      return food('picture_file_size')
    else
      0
    end

In some way, by passing function to `sum`, we have a simple effect of filter.
If we uses `filter`, we can write above query again

    r.db('foodb').table('foods').filter(function(food) {
      return food('picture_file_size').gt(1024 * 1024 * 4)
    }).sum('picture_file_size')
    //=>
    966379

In this case, we first find and return only documents where its
*picture_file_size* is greater than 4MB. Then we simply `sum` field
*picture_file_size* of all documents.

Basically, by passing function into `sum`, `avg`, we can transform document to
our desire result for doing `sum` or `avg` on it.

Doing some calculation is fun, but what if we want which food has smallest or
biggest picture file size. Let's move to `min` and `max`

## min and max

As their name, given a sequence, they find the minimum document. But how we
compare a JSON document? Therefore, we have to pass a *field name* to min,
the value of that field of document is used to compare among documents. If
we pass a function, the function is called on every document, the return value
is used to compare among documents.

    r.db('foodb').table('foods').max('picture_file_size').pluck('name', 'picture_file_size')
    //=>
    {
      "name":  "Meatball" ,
      "picture_file_size": 5102677
    }

We can also pass expression, the value of expression is used for comparing.
Let's find the compound which has most *health effect*.

As you can guess, RethinkDB often runs faster if we pass a field name because
no extra processing is made. In case of function, function has to be executed.
Example, let's try to get max file size of food in group *Type 1*.

    r.db('foodb').table('foods').max('picture_file_size').pluck('name', 'picture_file_size')
    //=>
    {
    "name":  "Meatball" ,
    "picture_file_size": 5102677
    }

In case of `min`, `max` they returns full document, but using a value to
compare. That hints that `min`, `max` may accept an index as comparing value.

Take this example. Try to find the *compounds* with bigest *msds_file_size*

    r.db('foodb').table('compounds').max('msds_file_size')
    //=>
    1 row returned in 217ms.

> Note that if you try `max` again without index, in second time the query
> run faster because a part of data was cached by RethinkDB. 
> The size of this cache is defined by this forumula
> (available_mem - 1024 MB) / 2.
> with available_mem is the memory when RethinkDB starts.

This runs on a SSD. Pretty slow. Now, see how fast it's compare with index

    r.db('foodb').table('compounds').indexCreate('msds_file_size')

Using this index to query, we can see it much faster.

    r.db('foodb').table('compounds').min({index: 'msds_file_size'})
    //=>
    1 row returned in 8ms.

By passing an secondary index to `min`, `max` function, the index value is used
to compare, run much faster and more efficient.

For complex logic, we can event pass a function to `min` or `max`, the return
values are used to compare. Let's find the food that has most compounds. The
compound of food is stored in *compound_foods* table.

    //First, let create an index, you can ignore if you created index before.
    r.db('foodb').table('compounds_foods').indexCreate('food_id')

    r.db('foodb').table('foods')
      .max(function(food) {
        return r.db('foodb').table('compounds_foods')
                .getAll(food('id'), {index: 'food_id'})
                .count()
      })
    //=>
    1 row returned in 1min 8.2

Yay, it runs in 1 minute and 8.2 seconds. Super slow. Because the function has
to be run on every document. That's being said, complex function may be slow,
but they are useful when we need it.

## distinct

Given a sequence, `distinct` remove duplication from it. When giving an index,
the duplication is detected by value of index. Its syntax is:

    sequence.distinct() → array
    table.distinct([:index => <indexname>]) → stream

As you can tell, whenever we return an array, we will run into 100,000 element
isues if the return array has more than 100,000 elements. So keep that in mind
and try to call distinct with a proper index, which we will learn quickly

Let's start with this simple example:

    r.expr([1, 2, 3, 4, 1]).distinct()
    //=> 4 rows returned
    [
      1 ,
      2 ,
      3 ,
      4
    ]

Let's get a list of our *username*, without duplication

    r.db("foodb")
      .table("users")
      .withFields("name")
      .distinct()
    //=>Executed in 30ms. 152 rows returned
    [
    {
    "name":  "Abe Willms"
    } ,
    {
    "name":  "Adela Klein V"
    } ,...]


Let's see what kind of `age` our users are:

    r.db("foodb")
      .table("users")
      .withFields("age")
      .distinct()
   //=> 71 rows are returned
    [
    {
    "age": 9
    } ,
    {
    "age": 10
    } ,
    {
    "age": 13
    } ,
    {
    "age": 14
    } ,]

So we have 152 unique user's name and 71 unique `age`.

Sometime, the way to detect duplication is not by comparing value of a single
field but the result of some logic. We can build an index based on that logic.
Then using value of index to run distinct on.

Imagine we want to divide user into 3 groups, depend on their *age*

    r.db("foodb")
      .table("users")
      .indexCreate("age-group", function (user) {
        return
          r.branch(
            user("age").lt(18),
            "teenager",
            r.branch(user("age").gt(50),
              "older",
              "adult"
            )
          )
      })

With that index, we can quickly list age group of our users, by
calling `distinct` on table, passing that index:

    r.db("foodb")
      .table("users")
      .distinct({index: "age-group"})
    //=> 3 rows return
    "adult"
    "older"
    "teenager"

When passing index, the value of index is returned; and therefore that value is
used to detect duplication.

Let's try on some big table: list all *orig_food_common_name* of *compound_foods*.

    r.db('foodb').table('compounds_foods')('orig_food_common_name').distinct()
    //=>
    9492 rows returned in 41.44s.

Here we use bracket to return only *orig_food_common_name* field, then remove
duplication with `distinct`. The query runs in **41.44** seconds. It also
returns the whole array, **with 9492 rows**, means all data has to be put into
memory and transfer over network. To make it faster, more efficient, we can
use index, and the result will be a stream.

Let's create an index for that field.

    r.db('foodb').table('compounds_foods').indexCreate('orig_food_common_name')

We passing an argument *index: index_name* to distinct

    r.db('foodb').table('compounds_foods').distinct({index: 'orig_food_common_name'})
    //=>40 rows returned in 161ms. Displaying rows 1-40, more available

We optimize it from *41.44s* to *161ms*!!! So fast. It has two reason for this
to be fast.

  1. We using an index as value for distinct to find the difference
  2. A stream is return. So the whole array won't have to load into memory and transfer to client

Basically, without an index, RethinkDB has to scan the whole table. It is slow in two ways:

  * slow to fetch data: read whole table, no index is used
  * slow to return data: a big amout of data transfer overnetwork from RethinkDB to client

When we pass an index, RethinkDB pickup the value of index, it doesn't have to
care or load the whole document. The return result is stream, so client receive
a cursor to fetch data lazily.

As you see, the command we learn on this chapter is operating on the whole
sequence. However, usually coming with aggregation is grouping. We want to
divide a sequence into many group, and doing aggregation on those group.
To do that, let's learn about group

## group

Yeah, group is everywhere. Group command groups data into many sub sequence,
we can continue to run aggeration on those sub sequence. For example, instead
of counting the whole sequence. We want to count how many document are in group
A, how many document in group B and so on, with group A or group B is document
that share same particular value.

Let's see how it is handle in RethinkDB.

    sequence.group(fieldOrFunction..., [{index: "indexName", multi: false}]) → grouped_stream

In a nut shell, taking a sequence, depend on the value of field or return value
of function, RethinkDB groups documents with same value into a group.

Looking at `flavors` table, let's group them by their `flavor_group` field:

    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
    #=>
    [{
        "group": "animal",
        "reduction": [
            {
                "category": "odor",
                "created_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561018,
                    "timezone": "-07:00"
                },
                "creator_id": null,
                "flavor_group": "animal",
                "id": 112,
                "name": "animal",
                "updated_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561018,
                    "timezone": "-07:00"
                },
                "updater_id": null
            }
        ]
    },
    {
        "group": "balsamic",
        "reduction": [
            {
                "category": "odor",
                "created_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561011,
                    "timezone": "-07:00"
                },
                "creator_id": null,
                "flavor_group": "balsamic",
                "id": 43,
                "name": "others",
                "updated_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561011,
                    "timezone": "-07:00"
                },
                "updater_id": null
            },
            {
                "category": "odor",
                "created_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561010,
                    "timezone": "-07:00"
                },
                "creator_id": null,
                "flavor_group": "balsamic",
                "id": 40,
                "name": "chocolate",
                "updated_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561010,
                    "timezone": "-07:00"
                },
                "updater_id": null
            }
        ]
    },
    {
        "group": "camphoraceous",
        "reduction": [
            {
                "category": "odor",
                "created_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561017,
                    "timezone": "-07:00"
                },
                "creator_id": null,
                "flavor_group": "camphoraceous",
                "id": 101,
                "name": "camphoraceous",
                "updated_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561017,
                    "timezone": "-07:00"
                },
                "updater_id": null
            }
        ]
    },
    ...]

The returned array includes two fields:

  * group: value of group field, in our case is value of `flavor_group` field
  * reduction: an array contains all of document has same value for `flavor_group` field

When we continue to chain function after `group`, the function will operate on `reduction` array.
The result of function will replaced the value of reduction array.

For example, we can count how many element of **reduction**:

    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
      .count()
    //=>
    [
    {
    "group": null ,
    "reduction": 743
    } ,
    {
    "group":  "animal" ,
    "reduction": 1
    } ,
    {
    "group":  "balsamic" ,
    "reduction": 10
    } ,
    {
    "group":  "camphoraceous" ,
    "reduction": 1
    } ,
    ...
    ]

So, the *reduction* field no longer contains an array of document, but contains
value of how many document in original reduction array.

T> ## Command chain after group runs on grouped array
T>
T> It's important to understand `group` make next function call operator
T> on its `reduction` field.

Similarly, instead of counting, say we care about the first document only.

    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
      .nth(0)
    //=>
    [
    {
    "group": null ,
    "reduction": {
    "category":  "odor" ,
    "created_at": Sun Oct 02 2011 06:12:18 GMT-07:00 ,
    "creator_id": null ,
    "flavor_group": null ,
    "id": 148 ,
    "name":  "cotton candy" ,
    "updated_at": Sun Oct 02 2011 06:12:18 GMT-07:00 ,
    "updater_id": null
    }
    } ,
    {
    "group":  "animal" ,
    "reduction": {
    "category":  "odor" ,
    "created_at": Sun Oct 02 2011 06:10:18 GMT-07:00 ,
    "creator_id": null ,
    "flavor_group":  "animal" ,
    "id": 112 ,
    "name":  "animal" ,
    "updated_at": Sun Oct 02 2011 06:10:18 GMT-07:00 ,
    "updater_id": null
    }
    } ,
    {
    "group":  "balsamic" ,
    "reduction": {
    "category":  "odor" ,
    "created_at": Sun Oct 02 2011 06:10:10 GMT-07:00 ,
    "creator_id": null ,
    "flavor_group":  "balsamic" ,
    "id": 40 ,
    "name":  "chocolate" ,
    "updated_at": Sun Oct 02 2011 06:10:10 GMT-07:00 ,
    "updater_id": null
    }
    } ,
    ...
    ]

Here, `nth(0)` will be call on *reduction* array, return its first element, and
re-assign the result to *reduction* field.

W> Why null group?
W>
W> Why do we have a value ***null*** here. It's because
W> some documents don't have any value for `flavor_group`,
W> or in other words, a NULL value. They are put into the same group
W> of NULL

Note that we have some limitations with `group` where the `group` size is 
over 100000 elements. For example, let's group `compounds_foods` by their
`orig_food_common_name`

    r.db("foodb")
      .table("compounds_foods")
      .group('orig_food_common_name')

And we got this:

    RqlRuntimeError: Grouped data over size limit `100000`.  Try putting a reduction (like `.reduce` or `.count`) on the end in:
      r.db("foodb").table("compounds_foods").group("orig_food_common_name")

Why so? Because when we end the chain with `group`, the whole array is loaded
into memory, and our sequence are greater than 100000 elements. We have around
~668K documents. However, when we call `reduce` or `count` on it, the number
of documen will be reduce. RethinkDB won't have to keep them all into memory and
makes it works.

Let's try what it suggests:

    r.db("foodb")
      .table("compounds_foods")
      .group('orig_food_common_name')
      .count()
    #=>
    //Executed in 45.03s. 9492 rows returned
    [
    {
        "group": null,
        "reduction": 4313
    },
    {
        "group": "AMARANTH FLAKES",
        "reduction": 68
    },
    ...
    ]

Now, this result is of course not what we expected. But why it runs? It is
because when we call `count`, the whole array of `reduction` field becomes
a single value instead of an array of grouped data, that makes the size of
final group data smaller. As you can see, ***9492*** is returned, and
that's all loaded into memory.

So keep in mind that we have some limitation with `group`. If you notice,
*9492* is the same number when we run `distinct` on *orig_food_common_name*
field.

    r.db('foodb').table('compounds_foods')('orig_food_common_name').distinct()
    //=>
    9492 rows returned in 41.02s.

They return same amount of document because while they are different, they
share same concept of equality. Look at its define a gain:

  * group: group many documents which has same value of field or function result into a single document.
  * distinct: eliminate duplication, based on the value of a field or function result.

While they return different data, they retruns same quantity of document.
*Distinct* turns.

This confirms that our query works properly. Sometimes, it's fun to go back
and try different query as a way to validate our queries.

## ungroup

As you can see, anything follow `group` operates on sub stream, or `reduction` array.
Can we make the follow function run on returned sequence of `group` itself? Such as,
we want to sort by the value of `reduction` field. Let's try to sort `flavors` by
how many value of `flavor_group`

    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
      .count()
      .ungroup()
      .orderBy(r.desc('reduction'))
    //=>
    [
    {
    "group": null ,
    "reduction": 743
    } ,
    {
    "group":  "fruity" ,
    "reduction": 24
    } ,
    {
    "group":  "floral" ,
    "reduction": 14
    } ,
    {
    "group":  "balsamic" ,
    "reduction": 10
    },...
    ]

So `ungroup` turns the return array from `group` into a sequence of object, with each object
includes 2 fields:

    * group
    * reduciton

and let any chain command follows it operate on this sequence.

Let's say we want to find top 10 fruits which has most flavors. Because we want
to eat fruits that is tasty, right?

We have a table *compounds_foods* with *food_id* and *compound_id*. Using
*food_id* we can know the food. Using *compound_id* we can know how many
flavors it has by querying on *compounds_flavors* table. Here we only care
abotut how many flavors, so we only needs to count, no need to get flavor
detail.



## reduce

I think at this point, you already know what `count()` does. From a sequence, or an array,
it returns a single number of how many items the sequence holds. So it transforms a whole
array into a single value. Unlike `map` which turns each element of sequence into
other value, and returns a new sequence with all return value from `map` function. 

`count` is an example of `reduce`. `reduce` accepts a function, let's call it `reduce_function`,
and produces a single value, by repeating a function with input is the previous output of 
`reduce_function`. The `reduce_function` can be called with those parameters:

  * two elements of sequence
  * one element of sequence and one result of previous reduce_function execution
  * two results of previous reductions.

We can say that, on the first execution, two first elements of 
sequence are passed into reduce function; on the second execution, one parameter is
third element of sequence, other parameter is the result of reduce function call on
first and second element. And so on for 4th, 5h execution...

But why do we have two results of previous reductions? It's because reduced function 
can run parallel across shards and CPU core, or even across computers in a cluster.
The final result of each reduce functions on each shards or each computer,
are then passed to reduce function again, to create final result.

But what will happen if the sequence has a single element? We don't have enough input
for reduce function. That is a special case, and RethinkDB will simply return 
value of element as result of reduce function.

Usually, `reduce` will be used with `map` to transform document into a value that can 
be aggregated. As you have seen, the reduce function paramters can be element of sequence, 
or the result of previous reduce function. Therefore, we will need `map` to
do some transformation so that the type of parameters and result of reduce function are
the same.

T> Tail call optimization
T>
T> You may already sense that `reduce` is similar to recursion. But recursion is bad, they
T> are quicjky fill up the heap. However, you know that we are also passing return
T> value to next call of `reduce` function. Therefore, the reduce function doesn't need to
T> know about state of a previous call. In other words, we don't have to store a stack
T> for previous function call. That's tail called optimization where we can reuse
T> current stack frame for function call instead of creating new one.
T> I think that under the hood, reduce function is implemented in that sense.

Let's rewrite `count` in `reduce` style. Giving it some thoughts, you know that the way we count thing
is by using variable; each time we get a new input, we increment that variable by 1,
until all of input are processed. The final value is how many element we have such as
counting in JavaScript:

    function count(sequence) {
      sum = 0
      for (i=0; i<=sequence.length; i++) {
        sum = sum + 1
      }
      return sum
    }
    count(['a', 'b', 'a1', 12, 'a9', 13])

In the above ***count*** function of JavaScript, we have a step where we initialize the
value of `sum` to 0, and a step where we increment `sum` to 1. 

Let's re-write it in a recursion way, so we can simulate `reduce` style:

    function count(element, sum) {
      if (typeof element == 'undefined') {
        return 
      }

      if (typeof element == 'undefined') {
        return 0
      }
      if (typeof sum == 'undefined') {
        sum = 0
      }
      count(
    }

    for (i=0; i<=sequence.length; i++) {
      
    }
    
    reduce_function = function(left, right) {
      return left+right
    }
    count_with_reduce(sequence, reduce_function(left, right) {
    })

    if (typeof sequence[0] == 'undefined') {
      return 0
    }

    if (typeof sequence[1] == 'undefined') {
      return 1
    }


# Wrap up

When finishing this chapter, you should know how to aggregation, how to group data, counting,
and call function on grouped data. Some keys thing:

  * try to use index if possible on `min`, `max`, `distinct`
  * without `ungroup`, any chain command works on sub stream group

