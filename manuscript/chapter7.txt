-# Aggregation

Any database has to offer some kind of aggregation functions, and RethinkDb is not
an exception here.

## sum, average, and count

A very normal task of a database is to get some kind of calculate from a given
sequence of data. You already know `count`, by calling it on a stream, you get
how many document it has. You can also passing a value or a function to `count`
so RethinkDB only count document match the value or when predicate function
return true.

For example, let count how many food is *Type 1* we have

    r.db('foodb').table('foods')('food_type').count('Type 1')
    //=>
    627

Here we are using bracket to fetch *food_type* field and count how many value
that match *Type 1*. We can also pass a ReQL expression or a function. Let's
find all food whose name starts with *L*

    r.db('foodb').table('foods').count(r.row('name').match('^L'))
    //=>
    31

Or using function:

    r.db('foodb').table('foods')
      .count(function(food){
        return food('name').match('^L')
        })
    //=>
    31

With some smart combination with `map`, we can `count` in different way:

    r.db('foodb').table('foods').map(r.row('name').split('').nth(0)).count('L')

Basically we get food name, split charater one by one using `split` command,
then call `nth(0)` to fet first document. Using `map` we transform food name
table into a stream of first charater of food name, then we count this stream
for how many element equal *L*.

`sum`, and `averag` is similar to `count` on how you use them. Just different
on what they give you. `sum` give you sum of sequence, and `average` does 
*average*. Let's find out how many bytes of storage need to store food image.
Each of document in *foods* table has a *picture_file_size* field store in
byte. 

    r.db('foodb').table('foods').sum('picture_file_size')
    //=>
    123463051

We can also sum direcly on the value of stream

    r.db('foodb').table('foods')('picture_file_size').sum()

The key thing is that you understand how those function operate. By default,
they operator on the whole document. That's why we have to use `sum('pictire_file_size')`
when we call `sum` direcly on table. However, when we already use bracket to
get the field, we can simply call `sum()` without any parameters.

    r.db('foodb').table('foods')('picture_file_size').sum()

So you can guess what is the average file size, let's find out it:

    r.db('foodb').table('foods')('picture_file_size').avg()
    //=>
    147155.0071513707

We can also passing a function to `sum` or `avg`. In that case, RethinkDb calls
the function on every document, then get the result and use them for sum
purpose.

Let's say we only interested in filesize which is bigger than 4MB

    r.db('foodb').table('foods').sum(function(food) {
      return r.branch(
       food('picture_file_size').gt(1024*1024*4),
       food('picture_file_size'),
        0)
    })
    //=>
    9666379

Here, we are using `branch` as a normal `if else` block:

    if food('picture_file_size') > 1024 * 1024 * 4
      return food('picture_file_size')
    else
      0
    end

In some way, by passing function to `sum`, we have a simple effect of filter.
If we uses `filter`, we can write above query again

    r.db('foodb').table('foods').filter(function(food) {
      return food('picture_file_size').gt(1024 * 1024 * 4)
    }).sum('picture_file_size')
    //=>
    966379

In this case, we first find and return only documents where its
*picture_file_size* is greater than 4MB. Then we simply `sum` field
*picture_file_size* of all documents.

Basically, by passing function into `sum`, `avg`, we can transform document to
our desire result for doing `sum` or `avg` on it.

Doing some calculation is fun, but what if we want which food has smallest or
biggest picture file size. Let's move to `min` and `max`

## min and max

As their name, given a sequence, they find the minimum document. But how we
compare a JSON document? Therefore, we have to pass a *field name* to min,
the value of that field of document is used to compare among documents. If
we pass a function, the function is called on every document, the return value
is used to compare among documents.

    r.db('foodb').table('foods').max('picture_file_size').pluck('name', 'picture_file_size')
    //=>
    {
      "name":  "Meatball" ,
      "picture_file_size": 5102677
    }

We can also pass expression, the value of expression is used for comparing.
Let's find the compound which has most *health effect*.

As you can guess, RethinkDB often runs faster if we pass a field name because
no extra processing is made. In case of function, function has to be executed.
Example, let's try to get max file size of food in group *Type 1*.

    r.db('foodb').table('foods').max('picture_file_size').pluck('name', 'picture_file_size')
    //=>
    {
    "name":  "Meatball" ,
    "picture_file_size": 5102677
    }

In case of `min`, `max` they returns full document, but using a value to
compare. That hints that `min`, `max` may accept an index as comparing value.

Take this example. Try to find the *compounds* with bigest *msds_file_size*

    r.db('foodb').table('compounds').max('msds_file_size')
    //=>
    1 row returned in 217ms.

> Note that if you try `max` again without index, in second time the query
> run faster because a part of data was cached by RethinkDB. 
> The size of this cache is defined by this forumula
> (available_mem - 1024 MB) / 2. 
> with available_mem is the memory when RethinkDB starts.

This runs on a SSD. Pretty slow. Now, see how fast it's compare with index

    r.db('foodb').table('compounds').indexCreate('msds_file_size')

Using this index to query:

    r.db('foodb').table('compounds').min({index: 'msds_file_size'})
    //=>
    1 row returned in 8ms.

A more complicated way to using `min`, `max` with index. Let's say we have.

## distinct

Distinct removed duplicate from a sequence. Let's start with this simple example:

    r.expr([1, 2, 3, 4, 1]).distinct()
    //=> 4 rows returned
    [
      1 ,
      2 ,
      3 ,
      4
    ]

Let's find all *orig_food_common_name* of *compound_foods*.

    r.db('foodb').table('compounds_foods')('orig_food_common_name').distinct()
    //=>
    9492 rows returned in 41.44s.

Here we use bracket to return only *orig_food_common_name* field, then remove
duplication with `distinct`. The query runs in **41.44** seconds. It also
returns the whole array, **with 9492 rows**, means all data has to be put into
memory and transfer over network. To make it faster, more efficient, we can 
use index, and the result will be a stream.

Let's create an index for that field.

    r.db('foodb').table('compounds_foods').indexCreate('orig_food_common_name')

We passing an argument *index: index_name* to distinct

    r.db('foodb').table('compounds_foods').distinct({index: 'orig_food_common_name'})

As you see, the command we learn on this chapter is operating on the whole
sequence. However, usually coming with aggregation is grouping. We want to
divide a sequence into many group, and doing aggregation on those group.
To do that, let's learn about group

## group

Yeah, group is everywhere. Let's see how it is handle in RethinkDB.

    sequence.group(fieldOrFunction..., [{index: "indexName", multi: false}]) → grouped_stream

In a nut shell, taking a sequence, depend on the value of field or return value
of function, RethinkDB groups documents with same value into a group.

Looking at `flavors` table, let's group them by their `flavor_group` field:

    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
    #=>
    [{
        "group": "animal",
        "reduction": [
            {
                "category": "odor",
                "created_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561018,
                    "timezone": "-07:00"
                },
                "creator_id": null,
                "flavor_group": "animal",
                "id": 112,
                "name": "animal",
                "updated_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561018,
                    "timezone": "-07:00"
                },
                "updater_id": null
            }
        ]
    },
    {
        "group": "balsamic",
        "reduction": [
            {
                "category": "odor",
                "created_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561011,
                    "timezone": "-07:00"
                },
                "creator_id": null,
                "flavor_group": "balsamic",
                "id": 43,
                "name": "others",
                "updated_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561011,
                    "timezone": "-07:00"
                },
                "updater_id": null
            },
            {
                "category": "odor",
                "created_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561010,
                    "timezone": "-07:00"
                },
                "creator_id": null,
                "flavor_group": "balsamic",
                "id": 40,
                "name": "chocolate",
                "updated_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561010,
                    "timezone": "-07:00"
                },
                "updater_id": null
            }
        ]
    },
    {
        "group": "camphoraceous",
        "reduction": [
            {
                "category": "odor",
                "created_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561017,
                    "timezone": "-07:00"
                },
                "creator_id": null,
                "flavor_group": "camphoraceous",
                "id": 101,
                "name": "camphoraceous",
                "updated_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561017,
                    "timezone": "-07:00"
                },
                "updater_id": null
            }
        ]
    },
    ...]

The returned array includes two fields:

  * group: value of group field, in our case is value of `flavor_group` field
  * reduction: an array contains all of document has same value for `flavor_group` field

When we continue to chain function after `group`, the function will operate on `reduction` array.
The result of function will replaced the value of reduction array.

For example, we can count how many element of **reduction**:

    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
      .count()
    //=>
    [
    {
    "group": null ,
    "reduction": 743
    } ,
    {
    "group":  "animal" ,
    "reduction": 1
    } ,
    {
    "group":  "balsamic" ,
    "reduction": 10
    } ,
    {
    "group":  "camphoraceous" ,
    "reduction": 1
    } ,
    ...
    ]

So, the *reduction* field no longer contains an array of document, but contains
value of how many document in original reduction array.

T> ## Command chain after group runs on grouped array
T>
T> It's important to understand `group` make next function call operator
T> on its `reduction` field.

Similarly, instead of counting, say we care about the first document only.

    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
      .nth(0)
    //=>
    [
    {
    "group": null ,
    "reduction": {
    "category":  "odor" ,
    "created_at": Sun Oct 02 2011 06:12:18 GMT-07:00 ,
    "creator_id": null ,
    "flavor_group": null ,
    "id": 148 ,
    "name":  "cotton candy" ,
    "updated_at": Sun Oct 02 2011 06:12:18 GMT-07:00 ,
    "updater_id": null
    }
    } ,
    {
    "group":  "animal" ,
    "reduction": {
    "category":  "odor" ,
    "created_at": Sun Oct 02 2011 06:10:18 GMT-07:00 ,
    "creator_id": null ,
    "flavor_group":  "animal" ,
    "id": 112 ,
    "name":  "animal" ,
    "updated_at": Sun Oct 02 2011 06:10:18 GMT-07:00 ,
    "updater_id": null
    }
    } ,
    {
    "group":  "balsamic" ,
    "reduction": {
    "category":  "odor" ,
    "created_at": Sun Oct 02 2011 06:10:10 GMT-07:00 ,
    "creator_id": null ,
    "flavor_group":  "balsamic" ,
    "id": 40 ,
    "name":  "chocolate" ,
    "updated_at": Sun Oct 02 2011 06:10:10 GMT-07:00 ,
    "updater_id": null
    }
    } ,
    ...
    ]

Here, `nth(0)` will be call on *reduction* array, return its first element, and
re-assign the result to *reduction* field.

W> Why null group?
W>
W> Why do we have a value ***null*** here. It's because
W> some documents don't have any value for `flavor_group`,
W> or in other words, a NULL value. They are put into the same group
W> of NULL

Note that we have some limitations with `group` where the `group` size is 
over 100000 elements. For example, let's group `compounds_foods` by their
`orig_food_common_name`

    r.db("foodb")
      .table("compounds_foods")
      .group('orig_food_common_name')

And we got this:

    RqlRuntimeError: Grouped data over size limit `100000`.  Try putting a reduction (like `.reduce` or `.count`) on the end in:
      r.db("foodb").table("compounds_foods").group("orig_food_common_name")

Why so? Because when we end the chain with `group`, the whole array is loaded
into memory, and our sequence are greater than 100000 elements. We have around
~668K documents. However, when we call `reduce` or `count` on it, the number
of documen will be reduce. RethinkDB won't them all into memory and makes it
works.

Let's try what it suggests:

    r.db("foodb")
      .table("compounds_foods")
      .group('orig_food_common_name')
      .count()
    #=>
    //Executed in 45.03s. 9492 rows returned
    [
    {
        "group": null,
        "reduction": 4313
    },
    {
        "group": "AMARANTH FLAKES",
        "reduction": 68
    },
    ...
    ]

Now, this result is of course not what we expected. But why it runs? It is
because when we call `count`, the whole array of `reduction` field becomes
a single value instead of an array of grouped data, that makes the size of
final group data smaller. As you can see, ***9492*** is returned, and
that's all loaded into memory.

So keep in mind that we have some limitation with `group`. If you notice,
*9492* is the same number when we run `distinct` on *orig_food_common_name*
field.

    r.db('foodb').table('compounds_foods')('orig_food_common_name').distinct()
    //=>
They returns same amount of document because they are just same concept.

  * group: group many document into a single document with a value of document
  * distinct: eliminate duplication, based on the value.

This confirms that our query works properly. Sometimes, it's fun to go back
and try different query as a way to validate our queries.



## ungroup

As you can see, anything follow `group` operates on sub stream, or `reduction` array.
Can we make the follow function run on returned sequence of `group` itself? Such as,
we want to sort by the value of `reduction` field. Let's try to sort `flavors` by
how many value of `flavor_group`

    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
      .count()
      .ungroup()
      .orderBy(r.desc('reduction'))
    //=>
    [
    {
    "group": null ,
    "reduction": 743
    } ,
    {
    "group":  "fruity" ,
    "reduction": 24
    } ,
    {
    "group":  "floral" ,
    "reduction": 14
    } ,
    {
    "group":  "balsamic" ,
    "reduction": 10
    },...
    ]

So `ungroup` turns the return array from `group` into a sequence of object, with each object
includes 2 fields:

    * group
    * reduciton

and let any chain command follows it operate on this sequence.

Let's say we want to find top 10 fruits which has most flavors. Because we want
to eat fruits that is tasty, right?

We have a table *compounds_foods* with *food_id* and *compound_id*. Using
*food_id* we can know the food. Using *compound_id* we can know how many
flavors it has by querying on *compounds_flavors* table. Here we only care
abotut how many flavors, so we only needs to count, no need to get flavor
detail.



## reduce

I think at this point, you already know what `count()` does. From a sequence, or an array,
it returns a single number of how many items the sequence holds. So it transforms a whole
array into a single value. Unlike `map` which turns each element of sequence into
other value, and returns a new sequence with all return value from `map` function. 

`count` is an example of `reduce`. `reduce` accepts a function, let's call it `reduce_function`,
and produces a single value, by repeating a function with input is the previous output of 
`reduce_function`. The `reduce_function` can be called with those parameters:

  * two elements of sequence
  * one element of sequence and one result of previous reduce_function execution
  * two results of previous reductions.

We can say that, on the first execution, two first elements of 
sequence are passed into reduce function; on the second execution, one parameter is
third element of sequence, other parameter is the result of reduce function call on
first and second element. And so on for 4th, 5h execution...

But why do we have two results of previous reductions? It's because reduced function 
can run parallel across shards and CPU core, or even across computers in a cluster.
The final result of each reduce functions on each shards or each computer,
are then passed to reduce function again, to create final result.

But what will happen if the sequence has a single element? We don't have enough input
for reduce function. That is a special case, and RethinkDB will simply return 
value of element as result of reduce function.

Usually, `reduce` will be used with `map` to transform document into a value that can 
be aggregated. As you have seen, the reduce function paramters can be element of sequence, 
or the result of previous reduce function. Therefore, we will need `map` to
do some transformation so that the type of parameters and result of reduce function are
the same.

T> Tail call optimization
T>
T> You may already sense that `reduce` is similar to recursion. But recursion is bad, they
T> are quicjky fill up the heap. However, you know that we are also passing return
T> value to next call of `reduce` function. Therefore, the reduce function doesn't need to
T> know about state of a previous call. In other words, we don't have to store a stack
T> for previous function call. That's tail called optimization where we can reuse
T> current stack frame for function call instead of creating new one.
T> I think that under the hood, reduce function is implemented in that sense.

Let's rewrite `count` in `reduce` style. Giving it some thoughts, you know that the way we count thing
is by using variable; each time we get a new input, we increment that variable by 1,
until all of input are processed. The final value is how many element we have such as
counting in JavaScript:

    function count(sequence) {
      sum = 0
      for (i=0; i<=sequence.length; i++) {
        sum = sum + 1
      }
      return sum
    }
    count(['a', 'b', 'a1', 12, 'a9', 13])

In the above ***count*** function of JavaScript, we have a step where we initialize the
value of `sum` to 0, and a step where we increment `sum` to 1. 

Let's re-write it in a recursion way, so we can simulate `reduce` style:

    function count(element, sum) {
      if (typeof element == 'undefined') {
        return 
      }

      if (typeof element == 'undefined') {
        return 0
      }
      if (typeof sum == 'undefined') {
        sum = 0
      }
      count(
    }

    for (i=0; i<=sequence.length; i++) {
      
    }
    
    reduce_function = function(left, right) {
      return left+right
    }
    count_with_reduce(sequence, reduce_function(left, right) {
    })

    if (typeof sequence[0] == 'undefined') {
      return 0
    }

    if (typeof sequence[1] == 'undefined') {
      return 1
    }

    

## distinct

Given a sequence, `distinct` remove duplication from it. When giving an index,
the duplication is detected by value of index. Its syntax is:

    sequence.distinct() → array
    table.distinct([:index => <indexname>]) → stream

As you can tell, whenever we return an array, we will run into 100,000 element
isues if the return array has more than 100,000 elements. So keep that in mind
and try to call distinct with a proper index.

Let's count how many distinct `user` we have:

    r.db("foodb")
      .table("users")
      .withFields("name")
      .distinct()
    //=>Executed in 30ms. 152 rows returned
    [
    {
    "name":  "Abe Willms"
    } ,
    {
    "name":  "Adela Klein V"
    } ,...]


Let's see what kind of `age` our users are:

    r.db("foodb")
      .table("users")
      .withFields("age")
      .distinct()
   //=> 71 rows are returned
    [
    {
    "age": 9
    } ,
    {
    "age": 10
    } ,
    {
    "age": 13
    } ,
    {
    "age": 14
    } ,]

So we have 152 unique user's name and 71 unique `age`. 


What will happen when using an index?  Let's try to create an index for user age, we 
divide them into 3 groups:

    r.db("foodb")
      .table("users")
      .indexCreate("age-group", function (user) {
        return
          r.branch(
            user("age").lt(18),
            "teenager",
            r.branch(user("age").gt(50),
              "older",
              "adult"
            )
          )   
      })

Calling `distinct` on table, passing that index:

    r.db("foodb")
      .table("users")
      .distinct({index: "age-group"})
    //=> 3 rows return
    "adult"
    "older"
    "teenager"

So when passing index, the value of index is returned; and therefore that value is
used to detect duplication.

## count

You already knew `count`. It also has an extra useful way to count is by passing a
value or a function. When passing a value, it counts the document whose value matches
the value such as counting users who are 18 year olds:

    r.db("foodb")
      .table("users")("age")
      .count(18)

When passing a function, it counts the document whose function evaulation returns true
on document, very similar to filter function. However, filter returns document while `count`
counts them such as counting users who are 19 years old and name starts with a K:

    r.db("foodb")
      .table("users")
      .count(function(user) {
        return user("age").eq(23).and(user("name").match("^L"))
      })
    //=>
    1

We can use `row` function too:

    r.db("foodb")
      .table("users")
      .count(r.row("rowage").eq(23).and(r.row("name").match("^L")))
    //=>
    1

We also have some other aggregation functions such as `sum`, `min`, `max` but
they are very simple, and you can read from API page in aggregation section[^Aggregation].

[^Aggregation]: http://www.rethinkdb.com/api/javascript/max/#

# Wrap up

When finishing this chapter, you should know how to aggregation, how to group data, counting,
and call function on grouped data.
