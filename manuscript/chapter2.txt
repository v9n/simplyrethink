-# Getting to know RethinkDB

Let's warm up with some RethinkDB concepts, ideas and tools.

# Getting Started

It's not uncommon to see someone write an interactive shell in browser for
evaluation purpose such as mongly, tryRedis. This isn't applied for RethinkDB
because it comes with an excellent editor where you can type code and run it.

Install RethinkDB by downloading package for your platform http://rethinkdb.com/docs/install/. Run it after installing.


## Ports

By default, RethinkDB runs on 3 ports

8080
: this is the web user interface of RethinkDB or the dashboard. You can query
the data, check performance and server status on that UI.

28015
: this is the client drive port. All client drive will connect to RethinkDB
through this port. If you remember in previous chapter, we used a `tcpdump`
command to listen on this port to capture data send over it.

29015
: this is intracluster port; different RethinkDB node in a cluster communicates
with eath others via this port

## The dashboard

Open your browser at http://127.0.0.1:8080 and welcome RethinkDB. You can play
around to see what you have:

Navigate to the Explrer tab, you can type the command from there. Let's start
with 

    r.dbList()

Run it and you can see a list of database.


# RethinkDB object

Similar to traditonal database system, we also have database in RethinkDB. A
database contains many tables. Each table contains your JSON document. Those
JSON documents can contain any field. A table doesn't force a schema for those
fields. 

A JSON document is similar to a row in MySQL. Each of the field in the document is
similar to column in MySQL. The document on a table doesn't have to contain some
fields, they can contain whatever.

# Durability

You will see an option/argument call `durability`
`*durability` appears a lot in many options of ReQL db. Because it's so
common and it's very important, I want to address it here. Durability accepts 
value of *'soft'* or *'hard'*. 

soft
: means the writes will be acknowledge by server immdediately and data will be flushed to disk in
background.

hard
: The opposite of soft. The default behaviour is to acknowledge after data is written to disk.
Therefore, when you don't need the data to be consitent, such as
writing a cache, or an important log, you should set durability to soft in order to
increase speed

# Atomicity

According to RethinkDB docs [^atomic], write atomicity is supported on a per-document basis. So when you write to a single document, it's either succesfully or nothing orrcur instead of updating a couple field and leaving your data in a bad shape. Furthermore, RethinkDB guarantees that any combination of operation can be executed in a single document will be write atomically. 

However, it does comes with a limit. To quote RethinkDB doc, Operations that cannot be proven deterministic cannot update the document in an atomic way. That being said, the unpredictable value won't be atomic. Eg, randome value, operation run by using JavaScript expression other than ReQL, or values which are fetched from somewhere else. RethinkDB will throw an error instead of silently do it or ignore. You can choose to set a flag for writing data in non-atomic way.

Multiple document writing isn't atomic.

[^atomic] http://www.rethinkdb.com/docs/architecture/#how-does-the-atomicity-model-work

# Command line tool

Besides the dashboard, RethinkDB gives us some command line utility to
interactive with it. Some of them are:

* import
* export
* dump
* restore

## import

In the spirit of giving users the dashboard, RethinkDB also gives us sample data.
You can download the data in file input_polls and country_stats at []https://github.com/rethinkdb/rethinkdb/tree/next/demos/election and import them into
test database

~~~~~~~~
rethinkdb import -c localhost:28015 --table test.input_polls --pkey uuid -f input_polls.json --format json
rethinkdb import -c localhost:28015 --table test.county_stats --pkey uuid -f county_stats.json --format json
~~~~~~~~

Notice the `--table` argument, we are passing the table name in format of
*database_name*.*table_name". In our case, we import the data into two tables:
`input_polls` and `county_stats` inside database `test`.

We will use those data set through out the book. At any point, if you happen to mess up the
data, just drop the table, and import again.

## export

Export exports your database into many JSON files, each file is a table. The
JSON file can be import using above import command.

## dump

dump will just export whole of data of a cluster. Its syntax

    rethinkdb dump -c 127.0.0.1:28015

You will see some status lines:
    
    NOTE: 'rethinkdb-dump' saves data and secondary indexes, but does *not* save
   cluster metadata.  You will need to recreate your cluster setup yourself after
   you run 'rethinkdb-restore'.
  Exporting to directory...
  [========================================] 100%
  428353 rows exported from 9 tables, with 0 secondary indexes
    Done (260 seconds)
  Zipping export directory...
    Done (25 seconds)

The dump result is a gzip file whose name is in format `rethinkdb_dump_{timestamp}.tar.gz`
It's very useful when you want to try out something and get back your original data. Note it here because you will need it later.

## restore

Once we got the dump file with `dump` command. We can restore with:

    rethinkdb restore -c 127.0.0.1:28015 rethinkdb_dump_DATE_TIME.tar.gz

# Import sample data

It's much nicer to work with real and fun data than boring data. I have used the
`dump` command, and you can restore them.

    rethinkdb restore -c 127.0.0.1:28015 simply_rethink_dump_2015-07-20T00:54:39.tar.gz


